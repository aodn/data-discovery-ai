{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model for Keyword Classification\n",
    "This notebook introduces (1) how we prepare and preprocess the datasets; (2) how we train and evaluate the ML model; and (3) how we use this trained ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query result from ElasticSearch with the following scripts, make sure the number of the size is larger than the real number of records so that can get all records.\n",
    "```\n",
    "    POST /es-indexer-edge/_search\n",
    "    {\n",
    "    \"size\": 11000,\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "    }\n",
    "```\n",
    "and to get the IMOS records only:\n",
    "```\n",
    "    POST /es-indexer-edge/_search\n",
    "    {\n",
    "    \"size\": 800,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "        \"must\": [\n",
    "            {\n",
    "            \"match\": {\n",
    "                \"providers.name\": \"IMOS\"\n",
    "            }\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: import necessory libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] =\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"./output/AODN_description.tsv\"\n",
    "KEYWORDS_DS = \"./output/AODN_parameter_vocabs.tsv\"\n",
    "TARGET_DS = \"./output/keywords_target.tsv\"\n",
    "VOCABS = ['AODN Discovery Parameter Vocabulary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET is a subset of the original source dataset, containing only the '_id', '_source.title', and '_source.description' columns. We retained these columns because we want to use '_source.description' as the feature X for the classification task. Therefore, we calculated the embeddings of the descriptions. Finally, we saved the processed dataset as a file for future use, as calculating embeddings is time-consuming, and saving/loading the file helps reduce this time overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(DATASET, sep='\\t')\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(KEYWORDS_DS, sep='\\t')\n",
    "ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(DATASET, sep='\\t')\n",
    "ds.describe()\n",
    "\n",
    "def get_description_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :] \n",
    "    return cls_embedding.squeeze().numpy()\n",
    "\n",
    "def calculate_embedding(ds):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=False)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tqdm.pandas()\n",
    "    ds['embedding'] = ds['_source.description'].progress_apply(lambda x: get_description_embedding(x, tokenizer, model))\n",
    "    return ds\n",
    "\n",
    "# saved_ds = calculate_embedding(ds)\n",
    "# save_to_file(ds, './output/AODN.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Prepare Target set\n",
    "\n",
    "The target set is the metadata records that we want to apply our trained ML model for predicting keywords, this is all non-categorised records. We apply the calculated embeddings for these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessor import load_from_file, save_to_file\n",
    "def get_target_ds():\n",
    "    target = pd.read_csv(TARGET_DS, sep='\\t')\n",
    "    aodn = load_from_file('./output/AODN.pkl')\n",
    "    aodn.columns = ['id', 'title', 'description', 'embedding']\n",
    "    merged_df = target.merge(aodn, on=['id', 'title','description'], how='left')\n",
    "    return merged_df\n",
    "\n",
    "target = get_target_ds()\n",
    "print(target.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the keywords for the target dataset are all empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan = target['keywords'].isnull().all()\n",
    "all_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Prepare train and test sets\n",
    "\n",
    "We prepare the train and test sets from the KEYWORDS_DS, which is the subset of AODN dataset that keywords using AODN vocabularies. We can check the keywords for the target dataset are all not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_ds = pd.read_csv(KEYWORDS_DS, sep='\\t')\n",
    "all_not_nan = keyword_ds['keywords'].notnull().all()\n",
    "all_not_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_ds.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We format the keywords field for better read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_formatter(text):\n",
    "    keywords = ast.literal_eval(text)\n",
    "    k_list = []\n",
    "    for keyword in keywords:\n",
    "        for concept in keyword['concepts']:\n",
    "            if keyword['title'] in VOCABS:\n",
    "                concept_str = keyword['title'] + ':' + concept['id']\n",
    "                k_list.append(concept_str)\n",
    "    return k_list\n",
    "\n",
    "def extract_labels(ds):\n",
    "    ds['keywords'] = ds['keywords'].apply(lambda x: keywords_formatter(x))\n",
    "    return ds\n",
    "\n",
    "formatted_keywords_ds = extract_labels(keyword_ds)\n",
    "print(formatted_keywords_ds['keywords'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply embedding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn = load_from_file('./output/AODN.pkl')\n",
    "aodn.columns = ['id', 'title', 'description', 'embedding']\n",
    "X_df = formatted_keywords_ds.merge(aodn, on=['id', 'title','description'], how='left')\n",
    "\n",
    "# save for further use\n",
    "save_to_file(X_df, './output/keyword_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_keywords_ds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the keywords field as the output Y. So we transfer the values in keywords from a list to a binary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(formatted_keywords_ds['keywords'])\n",
    "Y_df = pd.DataFrame(Y, columns=mlb.classes_)\n",
    "save_to_file(Y_df, './output/AODN_vocabs_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if there are any cell has value 1 in each row. This means the transform should be right and makes sure that item in Y has positive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_ones = (Y_df == 1).any(axis=1)\n",
    "print(f'Exist rows has no one values?:{(~rows_with_ones).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for further use\n",
    "save_to_file(Y_df, './output/keyword_target.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = load_from_file('./output/keyword_train.pkl')\n",
    "\n",
    "def split_data(ds):\n",
    "    print(f' ----------- \\n Shape: {ds.shape} \\n Columns{ds.columns} \\n ----------- ')\n",
    "\n",
    "    X = np.array(ds['embedding'].tolist())\n",
    "    Y = load_from_file('./output/AODN_vocabs_label.pkl')\n",
    "    Y_labels = Y.columns.tolist()\n",
    "\n",
    "    Y = Y.to_numpy()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)    \n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, Y_labels\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, Y_labels = split_data(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "INPUT_DIM = 768\n",
    "N_LABELS = 393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_model(X_train, Y_train, X_test, Y_test):\n",
    "    current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    model = Sequential([\n",
    "        Input(shape=(INPUT_DIM,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(N_LABELS, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "    epoch = 100\n",
    "    batch_size = 32\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test, Y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "    # history = model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size, class_weight=class_weights, validation_data=(X_test, Y_test))\n",
    "\n",
    "    model.save(f\"./output/saved/{current_time}-trained-keyword-epoch{epoch}-batch{batch_size}.keras\")\n",
    "\n",
    "    test_loss, test_accuracy, test_precision = model.evaluate(X_test, Y_test)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Test Precision: {test_precision}\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = keyword_model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.4\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = (predictions > confidence).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(Y_test, predictions):\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    hammingloss = hamming_loss(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions, average='micro')\n",
    "    recall = recall_score(Y_test, predictions, average='micro')\n",
    "    f1 = f1_score(Y_test, predictions, average='micro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'hammingloss': hammingloss,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trained_model = evaluation(Y_test=Y_test, predictions=predicted_labels)\n",
    "print(eval_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    predicted_keywords = [Y_labels[j] for j in range(len(predicted_labels[i])) if predicted_labels[i][j] == 1]\n",
    "    true_keywords = [Y_labels[j] for j in range(len(Y_test[i])) if Y_test[i][j] == 1]\n",
    "\n",
    "    print(f\"Predicted Labels: {predicted_keywords}\")\n",
    "    print(f\"True Labels: {true_keywords}\")\n",
    "    print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
