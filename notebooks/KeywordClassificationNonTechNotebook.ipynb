{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML model for Keyword Classification - Non-tech Notebook\n",
        "This notebook introduces (1) how to explore, prepare and preprocess the datasets; (2) how to train and evaluate the ML model; and (3) how to use this trained ML model, for non-technical audiences.\n",
        "## Problem Description\n",
        "In the catalogue $C = \\{M, K, P\\}$, a subset of metadata records, $M_t \\subseteq M$, have not yet been categorised with keywords. For these records, $K_i = \\emptyset $ for all $m_i \\in M_t$. Given another subset of metadata records, $M_s \\subseteq M$, where each record has already been categorised with keywords (i.e., $K_i \\neq \\emptyset $ for all $m_i \\in M_s$). The research question is as follows:\n",
        "\n",
        "How to design and develop a machine learning model, denoted as $MM_{keywords}$, that can automatically label the uncategorised metadata records $M_t$ using a predefined set of keywords $K$. Specifically, the model should be trained to learn a mapping rule $d_i \\mapsto K_i$ based on the observed patterns from the labelled metadata records $M_s$, where each description $d_i$ of a metadata record $m_i \\in M_s$ is associated with a set of keywords $K_i$. Once trained, the model should be able to apply this learned mapping to accurately categorise the records in $M_t$ by assigning their corresponding keywords based on the records' descriptions.\n",
        "\n",
        "To simplify the task, we restrict the scope of keywords to those falling within the primary AODN vocabulary:\n",
        "- AODN Discovery Parameter Vocabulary\n",
        "- AODN Instrument Vocabulary', 'AODN Discovery Parameter Vocabulary', 'AODN Platform Vocabulary'\n",
        "\n",
        "Only keywords $k_j \\in K_i$ that are part of the listed AODN vocabularies will be considered. Any keyword not belonging to these vocabularies will be excluded from $K_i$ for all metadata records in the categorised metadata set $M_s$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Dataset\n",
        "This section introduces how to load the target and sample sets, snippets to explore these sets, and prepare and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# add module path for notebook to use\n",
        "import sys\n",
        "import os\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\utils\")\n",
        "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\model\")\n",
        "\n",
        "current_path = os.getcwd()\n",
        "\n",
        "# import modules\n",
        "import preprocessor\n",
        "import keywordModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first load labelled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data_discovery_ai/input/keyword_sample.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m labelledSet \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data_discovery_ai/input/keyword_sample.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\yhu12\\OneDrive - University of Tasmania\\IMOS\\DataDiscovery\\data-discovery-ai\\data_discovery_ai\\utils\\preprocessor.py:29\u001b[0m, in \u001b[0;36mload_from_file\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     20\u001b[0m logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Saves an object to a file using pickle serialization. This function saves the specified object to a file in binary format. If a specific folder path is required, include it in the `file_name`.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m        obj: Any. The object to be saved; no type restriction.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m        file_name: str. The name of the file (including path if necessary) to save the object to.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Output:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m        None, not return any value in this function\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_file\u001b[39m(\n\u001b[0;32m     32\u001b[0m         obj: Any, \n\u001b[0;32m     33\u001b[0m         file_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     34\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_discovery_ai/input/keyword_sample.pkl'"
          ]
        }
      ],
      "source": [
        "labelledSet = preprocessor.load_from_file(\n",
        "        \"./data_discovery_ai/input/keyword_sample.pkl\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1631, 5)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labelledSet.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then prepare the input X and Y based on the labelled set"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
