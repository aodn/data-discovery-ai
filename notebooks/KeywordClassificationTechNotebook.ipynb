{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model for Keyword Classification - Tech Notebook\n",
    "This notebook introduces (1) how to explore, prepare and preprocess the datasets; (2) how to train and evaluate the ML model; and (3) how to use this trained ML model, for technical audiences.\n",
    "## Problem Description\n",
    "The AODN catalogue $C=\\{M, K, P\\}$ serves as a platform for storing datasets and their associated metadata. $M=\\{m_1,m_2,\\ldots, m_x\\}$ is a set of metadata records which are used to describe the dataset in AODN catalogue $C$. $K=\\{k_1, k_2, \\ldots, k_y\\}$ is a set of pre-defined keywords that are used to categorise dataset. In the catalogue $C = \\{M, K\\}$, a subset of metadata records, $M_t \\subseteq M$, have not yet been categorised with keywords. For these records, $K_i = \\emptyset $ for all $m_i \\in M_t$. Given another subset of metadata records, $M_s \\subseteq M$, where each record has already been categorised with keywords (i.e., $K_i \\neq \\emptyset $ for all $m_i \\in M_s$). The research question is as follows:\n",
    "\n",
    "How to design and develop a machine learning model, denoted as $MM_{keywords}$, that can automatically label the uncategorised metadata records $M_t$ using a predefined set of keywords $K$. Specifically, the model should be trained to learn a mapping rule $d_i \\mapsto K_i$ based on the observed patterns from the sample set $M_s$, where each description $d_i$ of a metadata record $m_i \\in M_s$ is associated with a set of keywords $K_i$. Once trained, the model should be able to apply this learned mapping to accurately categorise the records in $M_t$ by assigning their corresponding keywords based on the records' descriptions.\n",
    "\n",
    "To simplify the task, we restrict the scope of keywords to those falling within the primary AODN vocabulary:\n",
    "- AODN Instrument Vocabulary\n",
    "- AODN Discovery Parameter Vocabulary\n",
    "- AODN Platform Vocabulary\n",
    "\n",
    "Only keywords $k_j \\in K_i$ that are part of the listed AODN vocabularies will be considered. Any keyword not belonging to these vocabularies will be excluded from $K_i$ for all metadata records in the categorised metadata set $M_s$.\n",
    "\n",
    "### Formal Definitions\n",
    "- **Definition 1: A metadata record $m_i=(d_i, K_i), m_i \\in M$** is a record describing a dataset. Specifically, $i$ is the unique identifier of the record. $d_i$ is a textual abstract that serves as the description of the dataset. $K_i \\subseteq K$ is a subset of keywords used to label the dataset.\n",
    "- **Definition 2: A abstract $d_i$** is a piece of textual information which is used to describe the dataset. The embedding $\\mathbf{d_i}$ is a vector representation of the textual description $d_i$, calculated using the \"bert-base-uncased\" model. The embedding vector $\\mathbf{d_i}$ for each abstract $d_i$ has an universal dimensionality, denoted as $dim=|\\mathbf{d_i}|$. A feature matrix $\\mathbf{X}$ of a shape $|M_s| \\times dim$ aggregates the embeddings for the abstacts of all samples in $M_s$, where |M_s is the total number of metadata records.\n",
    "- **Definition 3: A keyword $k_j$** is a predefined label used for catogarising datasets. Each metadata record $m_i$ is associated with a set of keywords $K_i \\subseteq K$, while $K$ is the complete set of predefined keywords. The keywords $K_i$ for a metadata record $m_i$ is mathematiacally represented as a binary vector $y_i$ with a size of $|K|$. where each element indicates the presence or absence of a specific label. A value of 1 at position $j$ denotes the label $k_j \\in K$ is present in the metadata record $m_i$, in this sence $k_j \\in K_i$, while a value of 0 indicates its absence. A target matrix $\\mathbf{Y}$ is a $|M_s| \\times |K|$ binary matrix, where $|M_s|$ is the size of the metadata records set $M_s=\\{m_1,m_2,\\ldots, m_x\\}$, and $|K|$ is the size of the keywords set $K=\\{k_1, k_2, \\ldots, k_y\\}$. Each entry $ \\mathbf{K}[i, j] $ is 1 if metadata record $ m_i $ is associated with keyword $ k_j $, and 0 otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# add module path for notebook to use\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\utils\")\n",
    "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\model\")\n",
    "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\common\")\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# import modules\n",
    "import preprocessor\n",
    "import keywordModel\n",
    "import constants\n",
    "import es_connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the [framework](data-discovery-ai-framework.drawio.png), three distinct but connected modules work cooperatively as the keyword classifier pipeline. This notebook will go through the functions in these modules to show how we preprocess data, train the ML model, and make predictions.\n",
    "## Data Preprocessing\n",
    "The data preprocessing module is used to prepare data for training and testing models. Key features include: getting raw data, preparing sample data, converting textual data to numeric representations, resampling, and preparing input and output matrices.\n",
    "### Getting Raw Data\n",
    "Raw data means the all metadata records $M$ stored in Elasticsearch. A elasticsearch configuration file `esManager.ini` is needed to be created in folder `data_discoverty_ai/common`, in which two fields are required: `end_point` and `api_key`. For more information, please refer to [README](../README.md#file-structure). We first fetch raw data from Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Elasticsearch configuration\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "\n",
    "def load_es_config() -> configparser.ConfigParser:\n",
    "    elasticsearch_config_file_path = f\"../data_discovery_ai/common/{constants.ELASTICSEARCH_CONFIG}\"\n",
    "    esConfig = configparser.ConfigParser()\n",
    "    esConfig.read(elasticsearch_config_file_path)\n",
    "    return esConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect and query Elasticsearch\n",
    "esConfig = load_es_config()\n",
    "client = es_connector.connect_es(esConfig)\n",
    "index = os.getenv(\"ES_INDEX_NAME\", default=constants.ES_INDEX_NAME)\n",
    "raw_data = es_connector.search_es(client=client, index=index, batch_size=constants.BATCH_SIZE, sleep_time=constants.SLEEP_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **12943** metadata records in the staging environment. We can also check that there are **1721** items has no keyword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keyword_items = raw_data[raw_data['_source.themes'].apply(lambda x: x == [])]\n",
    "no_keyword_items_count = no_keyword_items.shape[0]\n",
    "no_keyword_items_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Samples\n",
    "Sample set is a subset of the raw dataset. A sample set $M_s$ is a set of metadata records in which keywords contain particular AODN vocabus. We first identify samples from raw data, and then preprocess the sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predefined vocabs\n",
    "def load_keyword_config() -> configparser.ConfigParser:\n",
    "    keyword_config_file_path = f\"../data_discovery_ai/common/{constants.KEYWORD_CONFIG}\"\n",
    "    keywordConfig = configparser.ConfigParser()\n",
    "    keywordConfig.read(keyword_config_file_path)\n",
    "    return keywordConfig\n",
    "keywordConfig = load_keyword_config()\n",
    "vocabs = keywordConfig[\"preprocessor\"][\"vocabs\"].split(\", \")\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified sample lables look like this format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify samples with predefined vocabs\n",
    "identified_sampleSet = preprocessor.identify_km_sample(raw_data, vocabs)\n",
    "identified_sampleSet.iloc[0][\"keywords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords is in a nested json format, we need to flattern them, and remove keywords which are not in the target vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_SampleSet = preprocessor.sample_preprocessor(identified_sampleSet, vocabs)\n",
    "preprocessed_SampleSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the sample set, for instance, row at index `20` has an empty keyword filed like `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sampleSet = preprocessed_SampleSet[preprocessed_SampleSet[\"keywords\"].apply(lambda x: x != [])]\n",
    "filtered_sampleSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate embeddings for the title and description field, which is used as the input feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSampleSet = preprocessor.calculate_embedding(filtered_sampleSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSampleSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train and Test Sets\n",
    "We now have the sample set with extra embedding information. We are going to split the sample set into train and test sets by preparing input feature matrix $X$ and output target matrix $Y$. The input feature matrix X is based on the embedding column, and the output Y is the mathmatic representation of the keyword column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Y_df, labelMap = preprocessor.prepare_X_Y(finalSampleSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared the input feature matrix `X` and the output target matrix `Y`. Additionally, we have `Y_df`, which includes column names for the `Y` matrix, and `labelMap`, which represents the keyword set of predefined keywords. In `labelMap`, the key is an encoded number corresponding to a column name in `Y_df`, and the value is a Concept object. We can review the details of a Concept object by its `to_json()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap.get(0).to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_label_index = preprocessor.identify_rare_labels(Y_df, constants.RARE_LABEL_THRESHOLD, list(labelMap.keys()))\n",
    "len(rare_label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that among 525 unique keywords, there are 332 keywords appears less than the `RARE_LABEL_THRESHOLD`. So we firstly duplicate records which have these rare labels with a customised resamplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oversampled, Y_oversampled = preprocessor.resampling(\n",
    "            X_train=X, Y_train=Y, strategy=\"custom\", rare_keyword_index=rare_label_index\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the sample size is increased from 647 to 1677 so that the records of rare labels are manually increased. We can now split the sample set to train and test sets follows a 80%-20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim, n_labels, X_train, Y_train, X_test, Y_test = (\n",
    "            preprocessor.prepare_train_test(X_oversampled, Y_oversampled, keywordConfig)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform oversampling only on the training set, as we want to avoid introducing training samples into the test set. This ensures the model does not encounter training data during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oversampled, Y_train_oversampled = preprocessor.resampling(\n",
    "            X_train=X_train, Y_train=Y_train, strategy=\"ROS\", rare_keyword_index=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we calculate the class weight, so that we can apply in model training by assigning majority classes lower weight, and minority classes higher weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weight_dict = keywordModel.get_class_weights(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have prepared all the data we need for training a keyword classification model. Let's move on to the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Model\n",
    "A model name is required for training a model. As mentioned in [README.md](../README.md), available options are: `development`,`experimental`, `staging`, `production`, `benchmark`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, history, model_name = keywordModel.keyword_model(\n",
    "            model_name=model_name,\n",
    "            X_train=X_train,\n",
    "            Y_train=Y_train,\n",
    "            X_test=X_test,\n",
    "            Y_test=Y_test,\n",
    "            class_weight=label_weight_dict,\n",
    "            dim=dim,\n",
    "            n_labels=n_labels,\n",
    "            params=keywordConfig,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = keywordConfig.getfloat(\"keywordModel\", \"confidence\")\n",
    "top_N = keywordConfig.getint(\"keywordModel\", \"top_N\")\n",
    "predicted_labels = keywordModel.prediction(\n",
    "    X_test, trained_model, confidence, top_N\n",
    ")\n",
    "eval = keywordModel.evaluation(\n",
    "    Y_test=Y_test, predictions=predicted_labels\n",
    ")\n",
    "eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a result of 94% precision, 92% recall, and 93% F1 score. Which is not bad. But we can still try different hypermeters to improve model performance. Please refer to [README.md](../README.md) to see hypermeter descriptions. To adjust model hypermeters, please go to file `data_discovery_ai\\common\\keyword_classification_parameters.ini` to try different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the trained model, let's use this model to make prediction. Let's assume we have a item entitled: *\"Corals and coral communities of Lord Howe Island, Australia\"* with an abstract *\"Ecological and taxonomic surveys of hermatypic scleractinian corals were carried out at approximately 100 sites around Lord Howe Island. Sixty-six of these sites were located on reefs in the lagoon, which extends for two-thirds of the length of the island on the western side. Each survey site consisted of a section of reef surface, which appeared to be topographically and faunistically homogeneous. The dimensions of the sites surveyed were generally of the order of 20m by 20m. Where possible, sites were arranged contiguously along a band up the reef slope and across the flat. The cover of each species was graded on a five-point scale of percentage relative cover. Other site attributes recorded were depth (minimum and maximum corrected to datum), slope (estimated), substrate type, total estimated cover of soft coral and algae (macroscopic and encrusting coralline). Coral data from the lagoon and its reef (66 sites) were used to define a small number of site groups which characterize most of this area.Throughout the survey, corals of taxonomic interest or difficulty were collected, and an extensive photographic record was made to augment survey data. A collection of the full range of form of all coral species was made during the survey and an identified reference series was deposited in the Australian Museum.In addition, less detailed descriptive data pertaining to coral communities and topography were recorded on 12 reconnaissance transects, the authors recording changes seen while being towed behind a boat.\n",
    " The purpose of this study was to describe the corals of Lord Howe Island (the southernmost Indo-Pacific reef) at species and community level using methods that would allow differentiation of community types and allow comparisons with coral communities in other geographic locations.\"* that is unlabelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_title = \"Corals and coral communities of Lord Howe Island, Australia\"\n",
    "item_abstract = \"\"\"Ecological and taxonomic surveys of hermatypic scleractinian corals were carried out at approximately 100 sites around Lord Howe Island. Sixty-six of these sites were located on reefs in the lagoon, which extends for two-thirds of the length of the island on the western side. Each survey site consisted of a section of reef surface, which appeared to be topographically and faunistically homogeneous. The dimensions of the sites surveyed were generally of the order of 20m by 20m. Where possible, sites were arranged contiguously along a band up the reef slope and across the flat. The cover of each species was graded on a five-point scale of percentage relative cover. Other site attributes recorded were depth (minimum and maximum corrected to datum), slope (estimated), substrate type, total estimated cover of soft coral and algae (macroscopic and encrusting coralline). Coral data from the lagoon and its reef (66 sites) were used to define a small number of site groups which characterize most of this area.Throughout the survey, corals of taxonomic interest or difficulty were collected, and an extensive photographic record was made to augment survey data. A collection of the full range of form of all coral species was made during the survey and an identified reference series was deposited in the Australian Museum.In addition, less detailed descriptive data pertaining to coral communities and topography were recorded on 12 reconnaissance transects, the authors recording changes seen while being towed behind a boat.\n",
    " The purpose of this study was to describe the corals of Lord Howe Island (the southernmost Indo-Pacific reef) at species and community level using methods that would allow differentiation of community types and allow comparisons with coral communities in other geographic locations.\"\"\"\n",
    "description = f\"{item_title}: {item_abstract}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first prepare input feature matrix X, which is the embedding of this description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embedding = preprocessor.get_description_embedding(description)\n",
    "dimension = description_embedding.shape[0]\n",
    "target_X = description_embedding.reshape(1, dimension)\n",
    "target_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML model is a probability model. The outputs are probabilities of labels presented in an item according to its title and abstract embeddings. We can check the output by load the pretrained model and print its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keywordModel.load_saved_model(model_name)\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.predict(target_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global parameters `confidence` and `top_N` are assigned in the `data_discovery_ai/common/keyword_classification_parameters.ini` configuration file.\n",
    "\n",
    "- The `confidence` parameter specifies the probability threshold. Probabilities exceeding this value indicate that the keyword is considered present in the item; otherwise, it is not.\n",
    "- The `top_N` parameter is used to select predicted keywords when no probability exceeds the confidence threshold. In this case, the top N keywords are selected and considered to appear in the item record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the trained model and X to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted_labels = keywordModel.prediction(\n",
    "        target_X,\n",
    "        trained_model,\n",
    "        keywordConfig.getfloat(\"keywordModel\", \"confidence\"),\n",
    "        keywordConfig.getint(\"keywordModel\", \"top_N\"),\n",
    "    )\n",
    "target_predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's in a binary format, but means that at which index the values are 1, the keywords at these index have a higher probability to be appeared in the item. So, we convert this binary array to readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = keywordModel.get_predicted_keywords(target_predicted_labels, labelMap)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this item has a most likely keyword `[{'vocab_type': 'AODN Discovery Parameter Vocabulary',\n",
    "  'value': 'abundance of biota',\n",
    "  'url': 'http://vocab.aodn.org.au/def/discovery_parameter/entity/488'},\n",
    " {'vocab_type': 'AODN Discovery Parameter Vocabulary',\n",
    "  'value': 'biotic taxonomic identification',\n",
    "  'url': 'http://vocab.aodn.org.au/def/discovery_parameter/entity/489'}]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
