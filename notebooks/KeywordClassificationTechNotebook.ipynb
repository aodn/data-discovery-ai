{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model for Keyword Classification - Tech Notebook\n",
    "This notebook introduces (1) how to explore, prepare and preprocess the datasets; (2) how to train and evaluate the ML model; and (3) how to use this trained ML model, for technical audiences.\n",
    "## Problem Description\n",
    "The AODN catalogue $C=\\{M, K, P\\}$ serves as a platform for storing datasets and their associated metadata. $M=\\{m_1,m_2,\\ldots, m_x\\}$ is a set of metadata records which are used to describe the dataset in AODN catalogue $C$. $K=\\{k_1, k_2, \\ldots, k_y\\}$ is a set of pre-defined keywords that are used to categorise dataset. In the catalogue $C = \\{M, K\\}$, a subset of metadata records, $M_t \\subseteq M$, have not yet been categorised with keywords. For these records, $K_i = \\emptyset $ for all $m_i \\in M_t$. Given another subset of metadata records, $M_s \\subseteq M$, where each record has already been categorised with keywords (i.e., $K_i \\neq \\emptyset $ for all $m_i \\in M_s$). The research question is as follows:\n",
    "\n",
    "How to design and develop a machine learning model, denoted as $MM_{keywords}$, that can automatically label the uncategorised metadata records $M_t$ using a predefined set of keywords $K$. Specifically, the model should be trained to learn a mapping rule $d_i \\mapsto K_i$ based on the observed patterns from the sample set $M_s$, where each description $d_i$ of a metadata record $m_i \\in M_s$ is associated with a set of keywords $K_i$. Once trained, the model should be able to apply this learned mapping to accurately categorise the records in $M_t$ by assigning their corresponding keywords based on the records' descriptions.\n",
    "\n",
    "To simplify the task, we restrict the scope of keywords to those falling within the primary AODN vocabulary:\n",
    "- AODN Instrument Vocabulary\n",
    "- AODN Discovery Parameter Vocabulary\n",
    "- AODN Platform Vocabulary\n",
    "\n",
    "Only keywords $k_j \\in K_i$ that are part of the listed AODN vocabularies will be considered. Any keyword not belonging to these vocabularies will be excluded from $K_i$ for all metadata records in the categorised metadata set $M_s$.\n",
    "\n",
    "### Formal Definitions\n",
    "- **Definition 1: A metadata record $m_i=(d_i, K_i), m_i \\in M$** is a record describing a dataset. Specifically, $i$ is the unique identifier of the record. $d_i$ is a textual abstract that serves as the description of the dataset. $K_i \\subseteq K$ is a subset of keywords used to label the dataset.\n",
    "- **Definition 2: A abstract $d_i$** is a piece of textual information which is used to describe the dataset. The embedding $\\mathbf{d_i}$ is a vector representation of the textual description $d_i$, calculated using the \"bert-base-uncased\" model. The embedding vector $\\mathbf{d_i}$ for each abstract $d_i$ has an universal dimensionality, denoted as $dim=|\\mathbf{d_i}|$. A feature matrix $\\mathbf{X}$ of a shape $|M_s| \\times dim$ aggregates the embeddings for the abstacts of all samples in $M_s$, where |M_s is the total number of metadata records.\n",
    "- **Definition 3: A keyword $k_j$** is a predefined label used for catogarising datasets. Each metadata record $m_i$ is associated with a set of keywords $K_i \\subseteq K$, while $K$ is the complete set of predefined keywords. The keywords $K_i$ for a metadata record $m_i$ is mathematiacally represented as a binary vector $y_i$ with a size of $|K|$. where each element indicates the presence or absence of a specific label. A value of 1 at position $j$ denotes the label $k_j \\in K$ is present in the metadata record $m_i$, in this sence $k_j \\in K_i$, while a value of 0 indicates its absence. A target matrix $\\mathbf{Y}$ is a $|M_s| \\times |K|$ binary matrix, where $|M_s|$ is the size of the metadata records set $M_s=\\{m_1,m_2,\\ldots, m_x\\}$, and $|K|$ is the size of the keywords set $K=\\{k_1, k_2, \\ldots, k_y\\}$. Each entry $ \\mathbf{K}[i, j] $ is 1 if metadata record $ m_i $ is associated with keyword $ k_j $, and 0 otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# add module path for notebook to use\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\utils\")\n",
    "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\model\")\n",
    "    sys.path.append(module_path+\"\\\\data_discovery_ai\\\\common\")\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# import modules\n",
    "import preprocessor\n",
    "import keywordModel\n",
    "import constants\n",
    "import es_connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the [framework](data-discovery-ai-framework.drawio.png), three distinct but connected modules work cooperatively as the keyword classifier pipeline. This notebook will go through the functions in these modules to show how we preprocess data, train the ML model, and make predictions.\n",
    "## Data Preprocessing\n",
    "The data preprocessing module is used to prepare data for training and testing models. Key features include: getting raw data, preparing sample data, converting textual data to numeric representations, resampling, and preparing input and output matrices.\n",
    "### Getting Raw Data\n",
    "Raw data means the all metadata records $M$ stored in Elasticsearch. A elasticsearch configuration file `esManager.ini` is needed to be created in folder `data_discoverty_ai/common`, in which two fields are required: `end_point` and `api_key`. For more information, please refer to [README](../README.md#file-structure). We first fetch raw data from Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Elasticsearch configuration\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "\n",
    "def load_es_config() -> configparser.ConfigParser:\n",
    "    elasticsearch_config_file_path = f\"../data_discovery_ai/common/{constants.ELASTICSEARCH_CONFIG}\"\n",
    "    esConfig = configparser.ConfigParser()\n",
    "    esConfig.read(elasticsearch_config_file_path)\n",
    "    return esConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "searching elasticsearch: 100%|██████████| 100/100 [02:05<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# connect and query Elasticsearch\n",
    "esConfig = load_es_config()\n",
    "client = es_connector.connect_es(esConfig)\n",
    "raw_data = es_connector.search_es(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Samples\n",
    "Sample set is a subset of the raw dataset. A sample set $M_s$ is a set of metadata records in which keywords contain particular AODN vocabus. We first identify samples from raw data, and then preprocess the sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AODN Instrument Vocabulary',\n",
       " 'AODN Discovery Parameter Vocabulary',\n",
       " 'AODN Platform Vocabulary']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predefined vocabs\n",
    "def load_keyword_config() -> configparser.ConfigParser:\n",
    "    keyword_config_file_path = f\"../data_discovery_ai/common/{constants.KEYWORD_CONFIG}\"\n",
    "    keywordConfig = configparser.ConfigParser()\n",
    "    keywordConfig.read(keyword_config_file_path)\n",
    "    return keywordConfig\n",
    "keywordConfig = load_keyword_config()\n",
    "vocabs = keywordConfig[\"preprocessor\"][\"vocabs\"].split(\", \")\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'concepts': [{'id': 'Practical salinity of the water body',\n",
       "    'url': 'http://vocab.nerc.ac.uk/collection/P01/current/PSLTZZ01'},\n",
       "   {'id': 'Temperature of the water body',\n",
       "    'url': 'http://vocab.nerc.ac.uk/collection/P01/current/TEMPPR01'},\n",
       "   {'id': 'Fluorescence of the water body',\n",
       "    'url': 'http://vocab.nerc.ac.uk/collection/P01/current/FLUOZZZZ'},\n",
       "   {'id': 'Turbidity of the water body',\n",
       "    'url': 'http://vocab.nerc.ac.uk/collection/P01/current/TURBXXXX'}],\n",
       "  'scheme': 'theme',\n",
       "  'description': '',\n",
       "  'title': 'AODN Discovery Parameter Vocabulary'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify samples with predefined vocabs\n",
    "labelledDS = preprocessor.identify_sample(raw_data, vocabs)\n",
    "labelledDS.iloc[0][\"keywords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords is in a nested json format, we need to flattern them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'concepts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessed_samples \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelledDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m preprocessed_samples\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\OneDrive - University of Tasmania\\IMOS\\DataDiscovery\\data-discovery-ai\\data_discovery_ai\\utils\\preprocessor.py:115\u001b[0m, in \u001b[0;36msample_preprocessor\u001b[1;34m(sampleSet, vocabs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_preprocessor\u001b[39m(sampleSet: pd\u001b[38;5;241m.\u001b[39mDataFrame, vocabs: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    Preprocess sample set data, including extract and reformat labels, and remove empty value records\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        cleaned_sampleSet: pd.Dataframe. The cleaned sample set\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     sampleSet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msampleSet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     list_lengths \u001b[38;5;241m=\u001b[39m sampleSet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m    119\u001b[0m     empty_keywords_records_index \u001b[38;5;241m=\u001b[39m list_lengths[list_lengths \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\AppData\\Local\\miniforge3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\OneDrive - University of Tasmania\\IMOS\\DataDiscovery\\data-discovery-ai\\data_discovery_ai\\utils\\preprocessor.py:116\u001b[0m, in \u001b[0;36msample_preprocessor.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_preprocessor\u001b[39m(sampleSet: pd\u001b[38;5;241m.\u001b[39mDataFrame, vocabs: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    Preprocess sample set data, including extract and reformat labels, and remove empty value records\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        cleaned_sampleSet: pd.Dataframe. The cleaned sample set\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     sampleSet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sampleSet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mkeywords_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    118\u001b[0m     list_lengths \u001b[38;5;241m=\u001b[39m sampleSet[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m    119\u001b[0m     empty_keywords_records_index \u001b[38;5;241m=\u001b[39m list_lengths[list_lengths \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\yhu12\\OneDrive - University of Tasmania\\IMOS\\DataDiscovery\\data-discovery-ai\\data_discovery_ai\\utils\\preprocessor.py:253\u001b[0m, in \u001b[0;36mkeywords_formatter\u001b[1;34m(text, vocabs)\u001b[0m\n\u001b[0;32m    251\u001b[0m k_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkeyword\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcepts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m keyword[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m vocabs \u001b[38;5;129;01mand\u001b[39;00m concept[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    255\u001b[0m             con \u001b[38;5;241m=\u001b[39m Concept(\n\u001b[0;32m    256\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mconcept[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    257\u001b[0m                 url\u001b[38;5;241m=\u001b[39mconcept[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    258\u001b[0m                 vocab_type\u001b[38;5;241m=\u001b[39mkeyword[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    259\u001b[0m             )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'concepts'"
     ]
    }
   ],
   "source": [
    "preprocessed_samples = preprocessor.sample_preprocessor(labelledDS, vocabs)\n",
    "preprocessed_samples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
