[![Language: Python](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)
[![CI](https://github.com/aodn/data-discovery-ai/actions/workflows/ci.yml/badge.svg)](https://github.com/aodn/data-discovery-ai/actions/workflows/ci.yml)

# Data Discovery AI
A consolidated framework for AI and ML components developed for the new portal.

## üîß Setup
### Environment variables
To run the app locally:
1. Go to the root folder of this project.
2. Create a .env file. (or simply copy the .env.sample file)
3. Add the following (with your real keys):
```shell
API_KEY="your_actual_api_key_here"
OPENAI_API_KEY="your_actual_openai_api_key_here"
PROFILE="your_actual_environment_here"
```
If you plan to train models, also include:
```shell
ES_ENDPOINT="your_actual_elasticsearch_endpoint"
ES_API_KEY="your_actual_es_api_key"
```

## üöÄ Running the App
### Option 1: Run with Docker (Recommended for Non-Developers)
1. Copy the content in .env.sample` to `.env` and fill in your keys.
2. Run
    ```shell
    ./startServer.sh
    ```
    Then visit [http://localhost:8000](http://localhost:8000)
3. Test App health at [http://localhost:8000/api/v1/ml/health](http://localhost:8000/api/v1/ml/health)

### Option 2: Run for Development (For Developers)
#### Conda (recommended for creating a virtual environment)
1. Install Conda (if not already installed):

   Follow the instructions at [Conda Installation](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).

2. Create Conda virtual environment:

   ```shell
   conda env create -f environment.yml
   ```

#### Dependencies management

Poetry is used for dependency management, the `pyproject.toml` file is what is the most important, it will orchestrate the project and its dependencies.

You can update the file `pyproject.toml` for adding/removing dependencies by using

```shell
poetry add <pypi-dependency-name> # e.g poetry add numpy
poetry remove <pypi-dependency-name> # e.g. poetry remove numpy
```

You might want to update the `poetry.lock` file after manually modifying `pyproject.toml` with `poetry lock` command. To update all dependencies, use `poetry update` command.

#### Installation and Usage

1. Activate Conda virtual environment:

   ```shell
   conda activate data-discovery-ai
   ```

2. Install environment dependencies:

   ```shell
   # after cloning the repo with git clone command
   cd data-discovery-ai
   poetry install
   ```

#### Before Usage
FastAPI runs internal checks before making `\process_record` API calls. These checks include:
1. ‚úÖ Required model resource files must be present in `data_discovery_ai/resources/`.
2. ‚úÖ A valid `OPENAI_API_KEY` must be in `.env` unless you're in `development` environment.
3. ‚úÖ If `PROFILE=development`, Ollama must be running locally at http://localhost:11434.

##### (Optional) Install Llama3 for Local Development
To use the Llama3 model locally without OpenAI:
1. Go to [Ollama download page](https://ollama.com/download) and download the version that matches your operating system (Windows, Linux, or macOS).
2. After installation, start Ollama either by launching the app or running the following command:
    ```shell
    ollama serve
    ```
3. Pull the "llama3" model used for local developmentÔºö
    ```shell
    ollama pull llama3
    ```

4. Consider install open-webui to run the llama3 for dev via:
    ```shell
    docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui ghcr.io/open-webui/open-webui:main
    ```

#### Run the FastAPI Server
Simply run:
   ```shell
   poetry run uvicorn data_discovery_ai.server:app --reload --log-config=log_config.yaml
   ```
#### Running Tests
Run all tests with:
    ```shell
    poetry run python -m unittest discover -s tests
    ```
#### Code formatting
Run manual checks:
```shell
pre-commit run --all-files
```
Checks are also executed when you run `git commit`. The configurations for pre-commit hooks are defined in `.pre-commit-config.yaml`.

#### Git Commit Guide (Optional)

We are using [gitmoji](https://gitmoji.dev/)(OPTIONAL) with husky and commitlint. Here you have an example of the most used ones:

- :art: - Improving structure/format of the code.
- :zap: - Improving performance.
- :fire: - Removing code or files.
- :bug: - Fixing a bug.
- :ambulance: - Critical hotfix.
- :sparkles: - Introducing new features.
- :memo: - Adding or updating documentation.
- :rocket: - Deploying stuff.
- :lipstick: - Updating the UI and style files.
- :tada: - Beginning a project.

Example of use:
`:wrench: add husky and commitlint config`

#### Branching name

- `hotfix/`: for quickly fixing critical issues,
- `usually/`: with a temporary solution
- `bugfix/`: for fixing a bug
- `feature/`: for adding, removing or modifying a feature
- `test/`: for experimenting something which is not an issue
- `wip/`: for a work in progress

And add the issue id after an `/` followed with an explanation of the task.

Example of use:
`feature/5348-create-react-app`

## ‚ñ∂Ô∏è Using the API
Once the app is running, two routes are available:

| **Route**                        | **Description**                                                   |
|----------------------------------|-------------------------------------------------------------------|
| `GET /api/v1/ml/health`          | Health check                                                      |
| `POST /api/v1/ml/process_record` | One single point for calling AI models to process metadata record |
### Example Request Body
```JSON
{
    "selected_model":["description_formatting"],
    "title": "test title",
    "abstract": "test abstract"
}
```

**Required Header**
```shell
X-API-Key: your_api_key
```

(Must match the value of `API_KEY` specified in the environment variables).

**AI Model Options**

- `selected_model`: the AI models provided by `data-discovery-ai`. It should be a list of strings, which are the name of the AI task agents. Currently, four AI task agents available for distinctive tasks:
    - `keyword_classification`: predict keywords from AODN vocabularies based on metadata `title` and `abstract` with pretrained ML model.
    - `delivery_classification`: predict data delivery mode based on metadata `title`, `abstract`, and `lineage` with pretrained ML model.
    - `description_formatting`: reformatting long abstract into Markdown format based on metadata `title` and `abstract` with LLM model "gpt-4o-mini".
    - `link_grouping`: categorising links into four groups: ["Python Notebook", "Document", "Data Access", "Other"] based on metadata `links`.

## ü§ñ Model Training (Optional)
Currently, two machine learning pipelines are available for training and evaluating models:

- `keyword`: keyword classification model, which is a Sequential model for multi-label classification task
- `delivery`: data delivery classification model, which is a self-learning model for binary classification task

### How to Run
To run one of the pipelines (for example, the keyword one), you can use the following command in your terminal:
```shell
python -m data_discovery_ai.ml.pipeline --pipeline keyword --start_from_preprocess False --model_name development
```
You can also use a shorter version:
```shell
 python -m data_discovery_ai.ml.pipeline -p keyword -s False -n experimental
```

### When Should I Re-Train?
If the raw data has changed (e.g., updated, cleaned, or expanded), you are recommended to re-train the model using the latest data.
To do this, set:
```shell
--start_from_preprocess True
```

As mentioned in [Environment variables](#environment-variables), ElasticSearch endpoint and API key are required to be set up in `.env` file.

### Track Training Progress with MLflow
We use [MLflow](https://mlflow.org/) to track model training and performance over time (like hypermeters, accuracy, precision, etc.).

To start the tracking server locally, run:
```shell
mlflow server --port 8080
```
Once it's running, you can open the tracking dashboard in your browser: [http://127.0.0.1:8080](http://127.0.0.1:8080)

You can change the model's training settings (like how long it trains or how fast it learns) by editing the trainer section in the file: `data_discovery_ai/config/parameters.yaml`

## üõ†Ô∏è Required Configuration Files
| **File**                                   | **Description**                                       |
|--------------------------------------------|-------------------------------------------------------|
| `data_discovery_ai/common/constants.py`    | 	Shared constants                                     |
| `data_discovery_ai/commom/parameters.yaml` | Store parameter settings for ML models and AI agents. |

## üìÅ Project Structure
```
data_discovery_ai/
‚îú‚îÄ‚îÄ config/             # Common utilities and shared configurations/constants used across modules‚îÇ
‚îú‚îÄ‚îÄ core/               # Core logic of the application such as API routes
‚îú‚îÄ‚îÄ agents/             # Task-specific agent modules using ML/AI/rule-based tools
‚îú‚îÄ‚îÄ ml/                 # Machine learning models: training, inference, evaluation logic
‚îú‚îÄ‚îÄ utils/              # Utility functions and helper scripts for various tasks
‚îú‚îÄ‚îÄ resources/          # Stored assets such as pretrained models, sample datasets, and other resources required for model inference
‚îú‚îÄ‚îÄ notebooks/          # Jupyter notebooks
‚îú‚îÄ‚îÄ tests/              # Unit tests for validating core components
‚îÇ   ‚îú‚îÄ‚îÄ agents
‚îÇ   ‚îú‚îÄ‚îÄ ml
‚îÇ   ‚îî‚îÄ‚îÄ utils
‚îú‚îÄ‚îÄ server.py             # FastAPI application entry point
```
